{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVgeUGizdDFZEYTOo47IBi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sulfika715/DeepLearning/blob/master/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3NbJbE6k21t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as f\n",
        "\n",
        "\n",
        "def calc_loss_acc(labels, logits):\n",
        "    \"\"\"Function to compute loss value. Here, we used cross entropy \n",
        "    Args:\n",
        "        logits: 4D tensor. output tensor from segnet model, which is the output of softmax\n",
        "        labels: true labels tensor\n",
        "    Returns:\n",
        "        loss (cross_entropy_mean), accuracy, predicts(logits with softmax) \n",
        "    \"\"\"\n",
        "    # calc cross entropy mean  cross_entropy\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='cross_entropy')\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
        "    tf.summary.scalar(name='loss', tensor=cross_entropy_mean)\n",
        "\n",
        "\n",
        "    predicts = tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1))\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(predicts, dtype=tf.float32))\n",
        "    tf.summary.scalar(name='accuracy', tensor=accuracy)\n",
        "\n",
        "    return cross_entropy_mean, accuracy, predicts\n",
        "\n",
        "\n",
        "def train_op(total_loss, global_steps, base_learning_rate, option='Adam'):\n",
        "    \"\"\"This function defines train optimizer \n",
        "    Args:\n",
        "        total_loss: the loss value\n",
        "        global_steps: global steps is used to track how many batch had been passed. In the training process, the initial value for global_steps = 0, here  \n",
        "        global_steps=tf.Variable(0, trainable=False). then after one batch of images passed, the loss is passed into the optimizer to update the weight, then the global \n",
        "        step increased by one.\n",
        "        base_learning_rate: default value 0.1\n",
        "    Returns:\n",
        "        the train optimizer\n",
        "    \"\"\"\n",
        "\n",
        "    # base_learning_rate = 0.01\n",
        "    # get update operation\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "\n",
        "        if option == 'Adam':\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=base_learning_rate)\n",
        "            print(\"Running with Adam Optimizer with learning rate:\", base_learning_rate)\n",
        "        elif option == 'SGD':\n",
        "            # base_learning_rate = 0.01\n",
        "            learning_rate_decay = tf.train.exponential_decay(base_learning_rate, global_steps, 1000, 0.0005)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(learning_rate_decay)\n",
        "            print(\"Running with SGD Optimizer with learning rate:\", learning_rate_decay)\n",
        "        else:\n",
        "            raise ValueError('Optimizer is not recognized')\n",
        "\n",
        "        grads = optimizer.compute_gradients(total_loss, var_list=tf.trainable_variables())\n",
        "        training_op = optimizer.apply_gradients(grads, global_step=global_steps)\n",
        "\n",
        "    return training_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGEDzyiUlgcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def max_pooling(inputs, ksize, stride, padding, name):\n",
        "    \"\"\"Create max pooling layer \n",
        "    Args:\n",
        "        inputs: float32 4D tensor\n",
        "        ksize: a tuple of 2 int with (kernel_height, kernel_width)\n",
        "        stride: a tuple\n",
        "        padding: string. padding mode 'SAME', '\n",
        "        name: string\n",
        "        \n",
        "    Returns:\n",
        "        4D tensor of [batch_size, height, width, channels]\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        value = tf.nn.max_pool(inputs, ksize=[1, ksize[0], ksize[1], 1], strides=[1, stride[0], stride[1], 1], padding=padding, name=scope.name)\n",
        "    return value\n",
        "\n",
        "def dropout(inputs, keep_prob, name):\n",
        "    \"\"\"Dropout layer\n",
        "    Args:\n",
        "        inputs: float32 4D tensor \n",
        "        keep_prob: the probability of keep training sample\n",
        "        name: layer name to define\n",
        "    Returns:\n",
        "        4D tensor of [batch_size, height, width, channels]\n",
        "    \"\"\"\n",
        "    \n",
        "    return tf.nn.dropout(inputs, rate=(1-keep_prob), name=name)\n",
        "\n",
        "\n",
        "def norm(inputs, radius=4, name=None):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        value = tf.nn.lrn(inputs, depth_radius=radius, bias=1.0, alpha=1e-4, beta=0.75, name=name)\n",
        "    return value\n",
        "\n",
        "# def batch_norm(inputs, name):\n",
        "#     \"\"\"batch normalization layer\n",
        "\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "def conv2d(inputs, shape, padding, name):\n",
        "    \"\"\"Create convolution 2D layer\n",
        "    Args:\n",
        "        inputs: float32. 4D tensor\n",
        "        shape: the shape of kernel \n",
        "        padding: string. padding mode 'SAME',\n",
        "        name: corressponding layer's name\n",
        "    Returns:\n",
        "        Output 4D tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        # get weights value and record a summary protocol buffer with a histogram\n",
        "        weights = tf.get_variable('weights', shape=shape, initializer=tf.initializers.he_normal())\n",
        "        tf.summary.histogram(scope.name + 'weights', weights)\n",
        "\n",
        "        # get biases value and record a summary protocol buffer with a histogram\n",
        "        biases = tf.get_variable('biases', shape=shape[3], initializer=tf.initializers.random_normal())\n",
        "        tf.summary.histogram(scope.name + 'biases', biases)\n",
        "        # compute convlotion W * X + b, activiation function relu function\n",
        "        outputs = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding=padding)\n",
        "        outputs = tf.nn.bias_add(outputs, biases)\n",
        "        outputs = tf.nn.relu(outputs, name=scope.name + 'relu')\n",
        "    return outputs\n",
        "\n",
        "def fc(inputs, shape, name):\n",
        "    \"\"\"Create fully collection layer \n",
        "    Args:\n",
        "        inputs: Float32. 2D tensor with shape [batch, input_units]\n",
        "        shape: Int. a tuple with [num_inputs, num_outputs]\n",
        "        name: sring. layer name\n",
        "    Returns:\n",
        "        Outputs fully collection tensor\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        weights = tf.get_variable('weights', shape = [shape[0], shape[1]], initializer=tf.initializers.he_normal())\n",
        "        biases = tf.get_variable('biases', shape = [shape[1]], initializer=tf.initializers.random_normal())\n",
        "        # outputs = tf.nn.xw_plus_b(inputs, weights, biases, name = scope.name)\n",
        "        outputs = tf.add(tf.matmul(inputs, weights), biases, name=scope.name)\n",
        "\n",
        "    return tf.nn.relu(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqvtxtKQlnuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def alexnet(inputs, num_classes, keep_prob):\n",
        "    \"\"\"Create alexnet model\n",
        "    \"\"\"\n",
        "    x = tf.reshape(inputs, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # first conv layer, downsampling layer, and normalization layer\n",
        "    conv1 = conv2d(x, shape=(11, 11, 1, 96), padding='SAME', name='conv1')\n",
        "    pool1 = max_pooling(conv1, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool1')\n",
        "    norm1 = norm(pool1, radius=4, name='norm1')\n",
        "\n",
        "    # second conv layer\n",
        "    conv2 = conv2d(norm1, shape=(5, 5, 96, 256), padding='SAME', name='conv2')\n",
        "    pool2 = max_pooling(conv2, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool2')\n",
        "    norm2 = norm(pool2, radius=4, name='norm2')\n",
        "\n",
        "    # 3rd conv layer\n",
        "    conv3 = conv2d(norm2, shape=(3, 3, 256, 384), padding='SAME', name='conv3')\n",
        "    # pool3 = max_pooling(conv3, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool3')\n",
        "    norm3 = norm(conv3, radius=4, name='norm3')\n",
        "\n",
        "    # 4th conv layer\n",
        "    conv4 = conv2d(norm3, shape=(3, 3, 384, 384), padding='SAME', name='conv4')\n",
        "\n",
        "    # 5th conv layer\n",
        "    conv5 = conv2d(conv4, shape=(3, 3, 384, 256), padding='SAME', name='conv5')\n",
        "    pool5 = max_pooling(conv5, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool5')\n",
        "    norm5 = norm(pool5, radius=4, name='norm5')\n",
        "\n",
        "    # first fully connected layer\n",
        "    fc1 = tf.reshape(norm5, shape=(-1, 4*4*256))\n",
        "    fc1 = fc(fc1, shape=(4*4*256, 4096), name='fc1')\n",
        "    fc1 = dropout(fc1, keep_prob=keep_prob, name='dropout1')\n",
        "\n",
        "    fc2 = fc(fc1, shape=(4096, 4096), name='fc2')\n",
        "    fc2 = dropout(fc2, keep_prob=keep_prob, name='dropout2')\n",
        "\n",
        "    # output logits value\n",
        "    with tf.variable_scope('classifier') as scope:\n",
        "        weights = tf.get_variable('weights', shape=[4096, num_classes], initializer=tf.initializers.he_normal())\n",
        "        biases = tf.get_variable('biases', shape=[num_classes], initializer=tf.initializers.random_normal())\n",
        "        # define output logits value\n",
        "        logits = tf.add(tf.matmul(fc2, weights), biases, name=scope.name + '_logits')\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mK3xHQalypF",
        "colab_type": "code",
        "outputId": "47a7a72e-8307-451e-c582-eae35b958fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "for name in list(flags.FLAGS):\n",
        "      delattr(flags.FLAGS,name)\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "flags.DEFINE_integer('valid_steps', 11, 'The number of validation steps ')\n",
        "\n",
        "flags.DEFINE_integer('max_steps', 1001, 'The number of maximum steps for traing')\n",
        "\n",
        "flags.DEFINE_integer('batch_size', 128, 'The number of images in each batch during training')\n",
        "\n",
        "flags.DEFINE_float('base_learning_rate', 0.0001, \"base learning rate for optimizer\")\n",
        "\n",
        "flags.DEFINE_integer('input_shape', 784, 'The inputs tensor shape')\n",
        "\n",
        "flags.DEFINE_integer('num_classes', 10, 'The number of label classes')\n",
        "\n",
        "flags.DEFINE_string('save_dir', './outputs', 'The path to saved checkpoints')\n",
        "\n",
        "flags.DEFINE_float('keep_prob', 0.75, \"the probability of keeping neuron unit\")\n",
        "\n",
        "flags.DEFINE_string('tb_path', './tb_logs/', 'The path points to tensorboard logs ')\n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(FLAGS):\n",
        "    \"\"\"Training model\n",
        "    \"\"\"\n",
        "    valid_steps = FLAGS.valid_steps\n",
        "    max_steps = FLAGS.max_steps\n",
        "    batch_size = FLAGS.batch_size\n",
        "    base_learning_rate = FLAGS.base_learning_rate\n",
        "    input_shape = FLAGS.input_shape  # image shape = 28 * 28\n",
        "    num_classes = FLAGS.num_classes\n",
        "    keep_prob = FLAGS.keep_prob\n",
        "    save_dir = FLAGS.save_dir\n",
        "    tb_path = FLAGS.tb_path \n",
        "\n",
        "    train_loss, train_acc = [], []\n",
        "    valid_loss, valid_acc = [], []\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    # define default tensor graphe \n",
        "    with tf.Graph().as_default():\n",
        "        images_pl = tf.placeholder(tf.float32, shape=[None, input_shape])\n",
        "        labels_pl = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
        "\n",
        "        # define a variable global_steps\n",
        "        global_steps = tf.Variable(0, trainable=False)\n",
        "\n",
        "        # build a graph that calculate the logits prediction from model\n",
        "        logits = alexnet(images_pl, num_classes, keep_prob)\n",
        "\n",
        "        loss, acc, _ = calc_loss_acc(labels_pl, logits)\n",
        "\n",
        "        # build a graph that trains the model with one batch of example and updates the model params \n",
        "        training_op = train_op(loss, global_steps, base_learning_rate)\n",
        "\n",
        "        # define the model saver\n",
        "        saver = tf.train.Saver(tf.global_variables())\n",
        "        \n",
        "        # define a summary operation \n",
        "        summary_op = tf.summary.merge_all()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            # print(sess.run(tf.trainable_variables()))\n",
        "            # start queue runners\n",
        "            coord = tf.train.Coordinator()\n",
        "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "            train_writter = tf.summary.FileWriter(tb_path, sess.graph)\n",
        "\n",
        "            # start training\n",
        "            for step in range(100):\n",
        "                # get train image / label batch\n",
        "                train_image_batch, train_label_batch = mnist.train.next_batch(batch_size)\n",
        "\n",
        "                train_feed_dict = {images_pl: train_image_batch, labels_pl: train_label_batch}\n",
        "\n",
        "                _, _loss, _acc, _summary_op = sess.run([training_op, loss, acc, summary_op], feed_dict = train_feed_dict)\n",
        "\n",
        "                # store loss and accuracy value\n",
        "                train_loss.append(_loss)\n",
        "                train_acc.append(_acc)\n",
        "                print(\"Iteration \" + str(step) + \", Mini-batch Loss= \" + \"{:.6f}\".format(_loss) + \", Training Accuracy= \" + \"{:.5f}\".format(_acc))\n",
        "\n",
        "                if step % 100 == 0:\n",
        "                    _valid_loss, _valid_acc = [], []\n",
        "                    print('Start validation process')\n",
        "\n",
        "                    for step in range(valid_steps):\n",
        "                        valid_image_batch, valid_label_batch = mnist.test.next_batch(batch_size)\n",
        "\n",
        "                        valid_feed_dict = {images_pl: valid_image_batch, labels_pl: valid_label_batch}\n",
        "\n",
        "                        _loss, _acc = sess.run([loss, acc], feed_dict = valid_feed_dict)\n",
        "\n",
        "                        _valid_loss.append(_loss)\n",
        "                        _valid_acc.append(_acc)\n",
        "\n",
        "                    valid_loss.append(np.mean(_valid_loss))\n",
        "                    valid_acc.append(np.mean(_valid_acc))\n",
        "\n",
        "                    print(\"Iteration {}: Train Loss {:6.3f}, Train Acc {:6.3f}, \n",
        "                    Val Loss {:6.3f}, Val Acc {:6.3f}\".format(step, train_loss[-1], train_acc[-1], valid_loss[-1], valid_acc[-1]))\n",
        "            \n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'train_loss'), train_loss)\n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'train_acc'), train_acc)\n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'valid_loss'), valid_loss)\n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'valid_acc'), valid_acc)\n",
        "            checkpoint_path = os.path.join(save_dir, 'model', 'model.ckpt')\n",
        "            saver.save(sess, checkpoint_path, global_step=step)\n",
        "\n",
        "            coord.request_stop()\n",
        "            coord.join(threads)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    train(FLAGS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From <ipython-input-2-5e219990e9bc>:13: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Running with Adam Optimizer with learning rate: 0.0001\n",
            "WARNING:tensorflow:From <ipython-input-7-80fe5f2fcbc9>:83: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "Iteration 0, Mini-batch Loss= 6.668061, Training Accuracy= 0.14062\n",
            "Start validation process\n",
            "Iteration 10: Train Loss  6.668, Train Acc  0.141, Val Loss  9.805, Val Acc  0.116\n",
            "Iteration 1, Mini-batch Loss= 10.615394, Training Accuracy= 0.10938\n",
            "Iteration 2, Mini-batch Loss= 8.362285, Training Accuracy= 0.12500\n",
            "Iteration 3, Mini-batch Loss= 8.804443, Training Accuracy= 0.09375\n",
            "Iteration 4, Mini-batch Loss= 8.171917, Training Accuracy= 0.14062\n",
            "Iteration 5, Mini-batch Loss= 6.760631, Training Accuracy= 0.11719\n",
            "Iteration 6, Mini-batch Loss= 4.454920, Training Accuracy= 0.14062\n",
            "Iteration 7, Mini-batch Loss= 4.343351, Training Accuracy= 0.15625\n",
            "Iteration 8, Mini-batch Loss= 3.232831, Training Accuracy= 0.14844\n",
            "Iteration 9, Mini-batch Loss= 3.080248, Training Accuracy= 0.16406\n",
            "Iteration 10, Mini-batch Loss= 2.999544, Training Accuracy= 0.17188\n",
            "Iteration 11, Mini-batch Loss= 3.393385, Training Accuracy= 0.10156\n",
            "Iteration 12, Mini-batch Loss= 3.085427, Training Accuracy= 0.16406\n",
            "Iteration 13, Mini-batch Loss= 2.917605, Training Accuracy= 0.17188\n",
            "Iteration 14, Mini-batch Loss= 2.976658, Training Accuracy= 0.14844\n",
            "Iteration 15, Mini-batch Loss= 2.791594, Training Accuracy= 0.17188\n",
            "Iteration 16, Mini-batch Loss= 2.638028, Training Accuracy= 0.17188\n",
            "Iteration 17, Mini-batch Loss= 2.847139, Training Accuracy= 0.18750\n",
            "Iteration 18, Mini-batch Loss= 2.567460, Training Accuracy= 0.15625\n",
            "Iteration 19, Mini-batch Loss= 2.323638, Training Accuracy= 0.25781\n",
            "Iteration 20, Mini-batch Loss= 2.302402, Training Accuracy= 0.24219\n",
            "Iteration 21, Mini-batch Loss= 2.268068, Training Accuracy= 0.23438\n",
            "Iteration 22, Mini-batch Loss= 2.184598, Training Accuracy= 0.28125\n",
            "Iteration 23, Mini-batch Loss= 2.161602, Training Accuracy= 0.32812\n",
            "Iteration 24, Mini-batch Loss= 2.025071, Training Accuracy= 0.32031\n",
            "Iteration 25, Mini-batch Loss= 1.986474, Training Accuracy= 0.25781\n",
            "Iteration 26, Mini-batch Loss= 1.868101, Training Accuracy= 0.36719\n",
            "Iteration 27, Mini-batch Loss= 1.949482, Training Accuracy= 0.32031\n",
            "Iteration 28, Mini-batch Loss= 1.965889, Training Accuracy= 0.34375\n",
            "Iteration 29, Mini-batch Loss= 1.653425, Training Accuracy= 0.45312\n",
            "Iteration 30, Mini-batch Loss= 1.723072, Training Accuracy= 0.45312\n",
            "Iteration 31, Mini-batch Loss= 1.436597, Training Accuracy= 0.52344\n",
            "Iteration 32, Mini-batch Loss= 1.636209, Training Accuracy= 0.45312\n",
            "Iteration 33, Mini-batch Loss= 1.604430, Training Accuracy= 0.46094\n",
            "Iteration 34, Mini-batch Loss= 1.333360, Training Accuracy= 0.53125\n",
            "Iteration 35, Mini-batch Loss= 1.558901, Training Accuracy= 0.44531\n",
            "Iteration 36, Mini-batch Loss= 1.485870, Training Accuracy= 0.54688\n",
            "Iteration 37, Mini-batch Loss= 1.331997, Training Accuracy= 0.53906\n",
            "Iteration 38, Mini-batch Loss= 1.520388, Training Accuracy= 0.49219\n",
            "Iteration 39, Mini-batch Loss= 1.332752, Training Accuracy= 0.51562\n",
            "Iteration 40, Mini-batch Loss= 1.121073, Training Accuracy= 0.63281\n",
            "Iteration 41, Mini-batch Loss= 1.140918, Training Accuracy= 0.59375\n",
            "Iteration 42, Mini-batch Loss= 1.089658, Training Accuracy= 0.63281\n",
            "Iteration 43, Mini-batch Loss= 1.005650, Training Accuracy= 0.70312\n",
            "Iteration 44, Mini-batch Loss= 0.979759, Training Accuracy= 0.64844\n",
            "Iteration 45, Mini-batch Loss= 0.994087, Training Accuracy= 0.65625\n",
            "Iteration 46, Mini-batch Loss= 0.986003, Training Accuracy= 0.65625\n",
            "Iteration 47, Mini-batch Loss= 0.816583, Training Accuracy= 0.75000\n",
            "Iteration 48, Mini-batch Loss= 0.778291, Training Accuracy= 0.74219\n",
            "Iteration 49, Mini-batch Loss= 0.692066, Training Accuracy= 0.77344\n",
            "Iteration 50, Mini-batch Loss= 1.071941, Training Accuracy= 0.68750\n",
            "Iteration 51, Mini-batch Loss= 0.549389, Training Accuracy= 0.81250\n",
            "Iteration 52, Mini-batch Loss= 0.637327, Training Accuracy= 0.76562\n",
            "Iteration 53, Mini-batch Loss= 0.752843, Training Accuracy= 0.71875\n",
            "Iteration 54, Mini-batch Loss= 0.582320, Training Accuracy= 0.81250\n",
            "Iteration 55, Mini-batch Loss= 0.769879, Training Accuracy= 0.79688\n",
            "Iteration 56, Mini-batch Loss= 0.567848, Training Accuracy= 0.81250\n",
            "Iteration 57, Mini-batch Loss= 0.565726, Training Accuracy= 0.79688\n",
            "Iteration 58, Mini-batch Loss= 0.548449, Training Accuracy= 0.83594\n",
            "Iteration 59, Mini-batch Loss= 0.638538, Training Accuracy= 0.82812\n",
            "Iteration 60, Mini-batch Loss= 0.580009, Training Accuracy= 0.80469\n",
            "Iteration 61, Mini-batch Loss= 0.556842, Training Accuracy= 0.82031\n",
            "Iteration 62, Mini-batch Loss= 0.651145, Training Accuracy= 0.82031\n",
            "Iteration 63, Mini-batch Loss= 0.474975, Training Accuracy= 0.85938\n",
            "Iteration 64, Mini-batch Loss= 0.604281, Training Accuracy= 0.82812\n",
            "Iteration 65, Mini-batch Loss= 0.503937, Training Accuracy= 0.82812\n",
            "Iteration 66, Mini-batch Loss= 0.537918, Training Accuracy= 0.82031\n",
            "Iteration 67, Mini-batch Loss= 0.482065, Training Accuracy= 0.83594\n",
            "Iteration 68, Mini-batch Loss= 0.460697, Training Accuracy= 0.87500\n",
            "Iteration 69, Mini-batch Loss= 0.568864, Training Accuracy= 0.81250\n",
            "Iteration 70, Mini-batch Loss= 0.416817, Training Accuracy= 0.87500\n",
            "Iteration 71, Mini-batch Loss= 0.457295, Training Accuracy= 0.85156\n",
            "Iteration 72, Mini-batch Loss= 0.473106, Training Accuracy= 0.88281\n",
            "Iteration 73, Mini-batch Loss= 0.514450, Training Accuracy= 0.90625\n",
            "Iteration 74, Mini-batch Loss= 0.358023, Training Accuracy= 0.89062\n",
            "Iteration 75, Mini-batch Loss= 0.535555, Training Accuracy= 0.82031\n",
            "Iteration 76, Mini-batch Loss= 0.418624, Training Accuracy= 0.88281\n",
            "Iteration 77, Mini-batch Loss= 0.577362, Training Accuracy= 0.81250\n",
            "Iteration 78, Mini-batch Loss= 0.478213, Training Accuracy= 0.85156\n",
            "Iteration 79, Mini-batch Loss= 0.347006, Training Accuracy= 0.89844\n",
            "Iteration 80, Mini-batch Loss= 0.345422, Training Accuracy= 0.88281\n",
            "Iteration 81, Mini-batch Loss= 0.471938, Training Accuracy= 0.87500\n",
            "Iteration 82, Mini-batch Loss= 0.392093, Training Accuracy= 0.87500\n",
            "Iteration 83, Mini-batch Loss= 0.419300, Training Accuracy= 0.82812\n",
            "Iteration 84, Mini-batch Loss= 0.378658, Training Accuracy= 0.87500\n",
            "Iteration 85, Mini-batch Loss= 0.497695, Training Accuracy= 0.85938\n",
            "Iteration 86, Mini-batch Loss= 0.492658, Training Accuracy= 0.82031\n",
            "Iteration 87, Mini-batch Loss= 0.420517, Training Accuracy= 0.87500\n",
            "Iteration 88, Mini-batch Loss= 0.482970, Training Accuracy= 0.86719\n",
            "Iteration 89, Mini-batch Loss= 0.352061, Training Accuracy= 0.88281\n",
            "Iteration 90, Mini-batch Loss= 0.340577, Training Accuracy= 0.91406\n",
            "Iteration 91, Mini-batch Loss= 0.365522, Training Accuracy= 0.89844\n",
            "Iteration 92, Mini-batch Loss= 0.455312, Training Accuracy= 0.84375\n",
            "Iteration 93, Mini-batch Loss= 0.431097, Training Accuracy= 0.86719\n",
            "Iteration 94, Mini-batch Loss= 0.297251, Training Accuracy= 0.87500\n",
            "Iteration 95, Mini-batch Loss= 0.438650, Training Accuracy= 0.85156\n",
            "Iteration 96, Mini-batch Loss= 0.316694, Training Accuracy= 0.90625\n",
            "Iteration 97, Mini-batch Loss= 0.475417, Training Accuracy= 0.83594\n",
            "Iteration 98, Mini-batch Loss= 0.388790, Training Accuracy= 0.86719\n",
            "Iteration 99, Mini-batch Loss= 0.372411, Training Accuracy= 0.86719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-80fe5f2fcbc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-80fe5f2fcbc9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(FLAGS)\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration {}: Train Loss {:6.3f}, Train Acc {:6.3f}, Val Loss {:6.3f}, Val Acc {:6.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './outputs/accuracy_loss/train_loss.npy'"
          ]
        }
      ]
    }
  ]
}